{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.integrate(calc_integrand_1(2), (t, 1 / 2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as spi\n",
    "from src.utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    true_bias: float\n",
    "    scoring_rule: str\n",
    "    base_cost: float\n",
    "    alpha: float\n",
    "    # num_trials: int\n",
    "    ell: float\n",
    "    # seed: int\n",
    "    start_decay_flips: int\n",
    "\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "\n",
    "def g_opt_inf(x):\n",
    "    return 3.0 * x**4 - 8.0 * x**3 + 6.0 * x**2\n",
    "\n",
    "\n",
    "# Generate random configurations\n",
    "scoring_rules = {\n",
    "    \"quadratic\": quadratic_scoring_rule,\n",
    "    \"logarithmic\": logarithmic_scoring_rule,\n",
    "    \"spherical\": spherical_scoring_rule,\n",
    "    \"optimal\": g_opt_inf,\n",
    "    # \"g_opt_1\": g_opt_1_poly,\n",
    "    # \"g_opt_2\": g_opt_2_poly,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def plot_data(df):\n",
    "    thresholds = [(\">30\", lambda x: x > 30), (\"<30\", lambda x: x < 30)]\n",
    "    rows = len(thresholds)\n",
    "    cols = len(scoring_rules)\n",
    "    \n",
    "    # Create color maps for each scoring rule\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    color_map = {rule: colors[i % len(colors)] for i, rule in enumerate(scoring_rules.keys())}\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=[\n",
    "            f\"{rule} Scoring Rule ({threshold_label} flips)\"\n",
    "            for threshold_label, _ in thresholds\n",
    "            for rule in scoring_rules.keys()\n",
    "        ],\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "\n",
    "    for row, (threshold_label, threshold_fn) in enumerate(thresholds):\n",
    "        # Calculate global min and max error values for consistent y-axis\n",
    "        all_errors = []\n",
    "        for rule, data in df.groupby(\"scoring_rule\"):\n",
    "            filtered_data = data[threshold_fn(data.flips)]\n",
    "            if not filtered_data.empty:\n",
    "                all_errors.extend(filtered_data.error.tolist())\n",
    "\n",
    "        y_min = min(all_errors) if all_errors else 0\n",
    "        y_max = max(all_errors) if all_errors else 1\n",
    "        \n",
    "        # Add some padding to the y-axis range\n",
    "        y_range = [y_min * 0.95, y_max * 1.05]\n",
    "\n",
    "        for i, (rule, data) in enumerate(df.groupby(\"scoring_rule\")):\n",
    "            filtered_data = data[threshold_fn(data.flips)]\n",
    "            \n",
    "            scatter = go.Scatter(\n",
    "                x=filtered_data.flips,\n",
    "                y=filtered_data.error,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=color_map[rule],\n",
    "                    size=8,\n",
    "                    opacity=0.7,\n",
    "                    line=dict(width=1, color='white')\n",
    "                ),\n",
    "                name=f\"{rule} ({threshold_label})\"\n",
    "            )\n",
    "\n",
    "            fig.add_trace(scatter, row=row + 1, col=i + 1)\n",
    "            \n",
    "            # Update axes with better formatting\n",
    "            fig.update_xaxes(\n",
    "                title_text=\"Number of Flips\",\n",
    "                row=row + 1, \n",
    "                col=i + 1,\n",
    "                gridcolor='lightgray',\n",
    "                showgrid=True\n",
    "            )\n",
    "            \n",
    "            fig.update_yaxes(\n",
    "                title_text=\"Error\",\n",
    "                row=row + 1, \n",
    "                col=i + 1, \n",
    "                range=y_range,\n",
    "                gridcolor='lightgray',\n",
    "                showgrid=True\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        width=300 * cols,\n",
    "        title_text=\"Error vs Number of Flips by Scoring Rule\",\n",
    "        title_x=0.5,\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        showlegend=False,\n",
    "        plot_bgcolor='rgba(240,240,240,0.2)',\n",
    "        paper_bgcolor='white',\n",
    "        margin=dict(l=60, r=40, t=80, b=60)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalized_scoring_rule(p):\n",
    "#     return unnormalized_scoring_rule(p) / integrate.quad(unnormalized_scoring_rule, 0, 1)[0]\n",
    "\n",
    "\n",
    "\n",
    "configs = []\n",
    "results = []\n",
    "from tqdm.auto import tqdm\n",
    "costs = np.linspace(0.00003, 0.1, 100)\n",
    "alphas = [0.9]\n",
    "for config_idx in tqdm(range(2000)):  # Generate 5 random configurations\n",
    "    rule_name = np.random.choice(list(scoring_rules.keys()))\n",
    "    cost = costs[np.random.randint(0, len(costs))]\n",
    "    alpha = alphas[np.random.randint(0, len(alphas))]\n",
    "\n",
    "    config = Config(\n",
    "        # true_bias=0.5 + 0.01 * np.random.uniform(-0.5, 0.5),\n",
    "        # true_bias=np.random.uniform(0.5, 0.5),\n",
    "        true_bias=np.random.random(),\n",
    "        base_cost=cost,\n",
    "        scoring_rule=rule_name,\n",
    "        alpha=alpha,  # Random alpha between 0.8 and 1.2\n",
    "        # alpha=1,  # Random alpha between 0.8 and 1.2\n",
    "        # num_trials=2000,  # Random number of trials\n",
    "        # ell=np.random.uniform(1.5, 3.0),  # Random ell between 1.5 and 3.0\n",
    "        ell=2,  # Random ell between 1.5 and 3.0\n",
    "        # seed=config_idx,  # Random seed\n",
    "        start_decay_flips=3,\n",
    "    )\n",
    "    error, flips = simulate_expert_flips_geometric_cost(\n",
    "        config.true_bias,\n",
    "        scoring_rules[config.scoring_rule],\n",
    "        config.base_cost,\n",
    "        alpha=config.alpha,\n",
    "        # num_trials=config.num_trials,\n",
    "        ell=config.ell,\n",
    "        # seed=config.seed,\n",
    "        start_decay_flips=config.start_decay_flips,\n",
    "    )\n",
    "    config = asdict(config)\n",
    "    config['error'] = error\n",
    "    config['flips'] = flips\n",
    "    results.append(config)\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# for config in tqdm(configs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, len(scoring_rules), figsize=(5 * len(scoring_rules), 10))\n",
    "\n",
    "fig.suptitle(\"Number of Flips vs True Bias by Scoring Rule\", fontsize=14, y=1.02)\n",
    "\n",
    "thresholds = [(\">30\", lambda x: x > 30), (\"<30\", lambda x: x < 30)]\n",
    "\n",
    "for row, (threshold_label, threshold_fn) in enumerate(thresholds):\n",
    "    for i, (rule, data) in enumerate(df.groupby(\"scoring_rule\")):\n",
    "        filtered_data = data[threshold_fn(data.flips)]\n",
    "        plt.sca(axes[row, i])\n",
    "        plt.hist2d(filtered_data.true_bias, filtered_data.flips, bins=100)\n",
    "        plt.colorbar(label=\"Count\")\n",
    "        plt.xlabel(\"True Bias\")\n",
    "        plt.ylabel(\"Number of Flips\")\n",
    "        plt.title(f\"{rule} Scoring Rule\\n({threshold_label} flips)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(\"Number of Flips vs Cost by Scoring Rule\", fontsize=14, y=1.02)\n",
    "\n",
    "thresholds = [(\">30\", lambda x: x > 30), (\"<30\", lambda x: x < 30)]\n",
    "\n",
    "for row, (threshold_label, threshold_fn) in enumerate(thresholds):\n",
    "    for i, (rule, data) in enumerate(df.groupby(\"scoring_rule\")):\n",
    "        filtered_data = data[threshold_fn(data.flips)]\n",
    "        plt.sca(axes[row, i])\n",
    "        plt.hist2d(filtered_data.flips, filtered_data.base_cost * filtered_data.flips, bins=50)\n",
    "        plt.colorbar(label=\"Count\")\n",
    "        plt.xlabel(\"Number of Flips\")\n",
    "        plt.ylabel(\"Total Cost\")\n",
    "        plt.title(f\"{rule} Scoring Rule\\n({threshold_label} flips)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Flip Count Distribution by Scoring Rule\")\n",
    "\n",
    "for i, (rule, data) in enumerate(df.groupby(\"scoring_rule\")):\n",
    "    sns.histplot(data=data, x=\"flips\", ax=axes[i])\n",
    "    \n",
    "    axes[i].set_title(f\"{rule} Scoring Rule\")\n",
    "    axes[i].set_xlabel(\"Number of Flips\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Alpha vs Error Distribution by Scoring Rule')\n",
    "\n",
    "for i, (rule, data) in enumerate(df.groupby('scoring_rule')):\n",
    "    sns.histplot(\n",
    "        data=data,\n",
    "        x='alpha',\n",
    "        y='error',\n",
    "        bins=20,\n",
    "        ax=axes[i],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[i].set_title(f'{rule} Scoring Rule')\n",
    "    axes[i].set_xlabel('Alpha')\n",
    "    axes[i].set_ylabel('Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_simulations(scoring_rule, base_cost=0.01, num_trials=1000, ell=2.0, seed=42):\n",
    "    \"\"\"Run simulations with constant, decreasing, and increasing cost structures.\"\"\"\n",
    "    # Constant cost (α=1)\n",
    "    const_error, const_flips, const_errors, const_flip_counts = (\n",
    "        simulate_expert_flips_geometric_cost(\n",
    "            scoring_rule,\n",
    "            base_cost,\n",
    "            alpha=1.0,\n",
    "            num_trials=num_trials,\n",
    "            ell=ell,\n",
    "            seed=seed,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Decreasing cost (α=0.9)\n",
    "    dec_error, dec_flips, dec_errors, dec_flip_counts = (\n",
    "        simulate_expert_flips_geometric_cost(\n",
    "            scoring_rule,\n",
    "            base_cost,\n",
    "            alpha=0.9,\n",
    "            num_trials=num_trials,\n",
    "            ell=ell,\n",
    "            seed=seed,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Increasing cost (α=1.1)\n",
    "    inc_error, inc_flips, inc_errors, inc_flip_counts = (\n",
    "        simulate_expert_flips_geometric_cost(\n",
    "            scoring_rule,\n",
    "            base_cost,\n",
    "            alpha=1.1,\n",
    "            num_trials=num_trials,\n",
    "            ell=ell,\n",
    "            seed=seed,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Constant\": (const_error, const_flips, const_errors, const_flip_counts),\n",
    "        \"Decreasing\": (dec_error, dec_flips, dec_errors, dec_flip_counts),\n",
    "        \"Increasing\": (inc_error, inc_flips, inc_errors, inc_flip_counts),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_comparison_results(results, scoring_rule_name):\n",
    "    \"\"\"Create bar plots comparing the three cost structures.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Extract data\n",
    "    cost_structures = list(results.keys())\n",
    "    errors = [results[cs][0] for cs in cost_structures]\n",
    "    flips = [results[cs][1] for cs in cost_structures]\n",
    "\n",
    "    # Error plot\n",
    "    bars1 = ax1.bar(cost_structures, errors, width=0.6)\n",
    "    ax1.set_ylabel(\"Mean Error\", fontsize=12)\n",
    "    ax1.set_title(\n",
    "        f\"Mean Error by Cost Structure\\n({scoring_rule_name} Scoring Rule)\", fontsize=14\n",
    "    )\n",
    "    ax1.set_ylim(0, max(errors) * 1.2)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.0001,\n",
    "            f\"{height:.4f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Flips plot\n",
    "    bars2 = ax2.bar(cost_structures, flips, width=0.6)\n",
    "    ax2.set_ylabel(\"Average Number of Flips\", fontsize=12)\n",
    "    ax2.set_title(\n",
    "        f\"Average Flips by Cost Structure\\n({scoring_rule_name} Scoring Rule)\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    ax2.set_ylim(0, max(flips) * 1.2)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.1,\n",
    "            f\"{height:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_distribution_comparison(results, scoring_rule_name):\n",
    "    \"\"\"Create violin plots showing the distribution of errors and flips.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Extract data for violin plots\n",
    "    cost_structures = list(results.keys())\n",
    "    error_data = [results[cs][2] for cs in cost_structures]\n",
    "    flips_data = [results[cs][3] for cs in cost_structures]\n",
    "\n",
    "    # Error distribution plot\n",
    "    ax1.violinplot(error_data, showmeans=True)\n",
    "    ax1.set_xticks(range(1, len(cost_structures) + 1))\n",
    "    ax1.set_xticklabels(cost_structures)\n",
    "    ax1.set_ylabel(\"Error Distribution\", fontsize=12)\n",
    "    ax1.set_title(\n",
    "        f\"Error Distribution by Cost Structure\\n({scoring_rule_name} Scoring Rule)\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    # Flips distribution plot\n",
    "    ax2.violinplot(flips_data, showmeans=True)\n",
    "    ax2.set_xticks(range(1, len(cost_structures) + 1))\n",
    "    ax2.set_xticklabels(cost_structures)\n",
    "    ax2.set_ylabel(\"Number of Flips Distribution\", fontsize=12)\n",
    "    ax2.set_title(\n",
    "        f\"Flip Count Distribution by Cost Structure\\n({scoring_rule_name} Scoring Rule)\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_alpha_sweep(results, scoring_rule_name):\n",
    "    \"\"\"Plot error and flips as a function of alpha.\"\"\"\n",
    "    alphas = sorted(results.keys())\n",
    "    errors = [results[a][0] for a in alphas]\n",
    "    flips = [results[a][1] for a in alphas]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Mark the constant cost case (α=1)\n",
    "    const_idx = alphas.index(1.0)\n",
    "\n",
    "    # Error plot\n",
    "    ax1.plot(alphas, errors, marker=\"o\", linewidth=2)\n",
    "    ax1.axvline(\n",
    "        x=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Constant Cost (α=1)\"\n",
    "    )\n",
    "    ax1.scatter([1.0], [errors[const_idx]], color=\"red\", s=100, zorder=5)\n",
    "    ax1.set_xlabel(\"Alpha (α)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Mean Error\", fontsize=12)\n",
    "    ax1.set_title(\n",
    "        f\"Mean Error vs. Alpha\\n({scoring_rule_name} Scoring Rule)\", fontsize=14\n",
    "    )\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Annotate regions\n",
    "    ax1.text(\n",
    "        alphas[0],\n",
    "        max(errors),\n",
    "        \"Decreasing Cost\",\n",
    "        color=\"blue\",\n",
    "        fontsize=12,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    ax1.text(\n",
    "        alphas[-1],\n",
    "        max(errors),\n",
    "        \"Increasing Cost\",\n",
    "        color=\"blue\",\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # Flips plot\n",
    "    ax2.plot(alphas, flips, marker=\"o\", linewidth=2)\n",
    "    ax2.axvline(\n",
    "        x=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Constant Cost (α=1)\"\n",
    "    )\n",
    "    ax2.scatter([1.0], [flips[const_idx]], color=\"red\", s=100, zorder=5)\n",
    "    ax2.set_xlabel(\"Alpha (α)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Average Number of Flips\", fontsize=12)\n",
    "    ax2.set_title(\n",
    "        f\"Average Flips vs. Alpha\\n({scoring_rule_name} Scoring Rule)\", fontsize=14\n",
    "    )\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "\n",
    "    # Annotate regions\n",
    "    ax2.text(\n",
    "        alphas[0],\n",
    "        max(flips),\n",
    "        \"Decreasing Cost\",\n",
    "        color=\"blue\",\n",
    "        fontsize=12,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    ax2.text(\n",
    "        alphas[-1],\n",
    "        max(flips),\n",
    "        \"Increasing Cost\",\n",
    "        color=\"blue\",\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run the main simulation comparing cost structures\n",
    "def main():\n",
    "    # Set parameters\n",
    "    base_cost = 0.1\n",
    "    num_trials = 2000\n",
    "    ell = 2.0\n",
    "    seed = 42\n",
    "\n",
    "    # Run simulations for different scoring rules\n",
    "    print(\"Running simulations for Quadratic scoring rule...\")\n",
    "    quadratic_results = run_simulations(\n",
    "        quadratic_scoring_rule, base_cost, num_trials, ell, seed\n",
    "    )\n",
    "\n",
    "    print(\"Running simulations for Logarithmic scoring rule...\")\n",
    "    log_results = run_simulations(\n",
    "        logarithmic_scoring_rule, base_cost, num_trials, ell, seed\n",
    "    )\n",
    "\n",
    "    print(\"Running simulations for Spherical scoring rule...\")\n",
    "    spherical_results = run_simulations(\n",
    "        spherical_scoring_rule, base_cost, num_trials, ell, seed\n",
    "    )\n",
    "\n",
    "    # Create comparison plots\n",
    "    print(\"Creating comparison plots...\")\n",
    "    plot_comparison_results(quadratic_results, \"Quadratic\")\n",
    "    plot_distribution_comparison(quadratic_results, \"Quadratic\")\n",
    "\n",
    "    plot_comparison_results(log_results, \"Logarithmic\")\n",
    "    plot_distribution_comparison(log_results, \"Logarithmic\")\n",
    "\n",
    "    plot_comparison_results(spherical_results, \"Spherical\")\n",
    "    plot_distribution_comparison(spherical_results, \"Spherical\")\n",
    "\n",
    "    # Run alpha sweep for more detailed analysis\n",
    "    print(\"Running alpha sweep analysis...\")\n",
    "    alphas = [0.7, 0.8, 0.9, 0.95, 1.0, 1.05, 1.1, 1.2, 1.3]\n",
    "\n",
    "    quadratic_alpha_results = run_alpha_sweep(\n",
    "        quadratic_scoring_rule, alphas, base_cost, num_trials, ell, seed\n",
    "    )\n",
    "    plot_alpha_sweep(quadratic_alpha_results, \"Quadratic\")\n",
    "\n",
    "    log_alpha_results = run_alpha_sweep(\n",
    "        logarithmic_scoring_rule, alphas, base_cost, num_trials, ell, seed\n",
    "    )\n",
    "    plot_alpha_sweep(log_alpha_results, \"Logarithmic\")\n",
    "\n",
    "    spherical_alpha_results = run_alpha_sweep(\n",
    "        spherical_scoring_rule, alphas, base_cost, num_trials, ell, seed\n",
    "    )\n",
    "    plot_alpha_sweep(spherical_alpha_results, \"Spherical\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed results\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for rule_name, results in [\n",
    "        (\"Quadratic\", quadratic_results),\n",
    "        (\"Logarithmic\", log_results),\n",
    "        (\"Spherical\", spherical_results),\n",
    "    ]:\n",
    "        print(f\"\\n{rule_name} Scoring Rule:\")\n",
    "        for cost_type, (error, flips, _, _) in results.items():\n",
    "            print(f\"  {cost_type} Cost: Error = {error:.4f}, Avg Flips = {flips:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some common scoring rules\n",
    "def log_score(p: float) -> float:\n",
    "    return np.log(p)\n",
    "\n",
    "\n",
    "def quadratic_scoring_rule(p: float) -> float:\n",
    "    \"\"\"Quadratic scoring rule: 2p - (p^2 + (1-p)^2)\"\"\"\n",
    "    return 2 * p - (p**2 + (1 - p) ** 2)\n",
    "\n",
    "\n",
    "def logarithmic_scoring_rule(p: float) -> float:\n",
    "    \"\"\"Logarithmic scoring rule: ln(p)\"\"\"\n",
    "    return np.log(p)\n",
    "\n",
    "\n",
    "def spherical_scoring_rule(p: float) -> float:\n",
    "    \"\"\"Spherical scoring rule: p/sqrt(p^2 + (1-p)^2)\"\"\"\n",
    "    return p / np.sqrt(p**2 + (1 - p) ** 2)\n",
    "\n",
    "\n",
    "def optimal_scoring_rule(p: float, ell: float = 2.0) -> float:\n",
    "    \"\"\"\n",
    "    Optimal scoring rule g_{ell,Opt} from the paper.\n",
    "    Default ell=2 minimizes expected squared error.\n",
    "    \"\"\"\n",
    "    if p <= 0.5:\n",
    "        return -(((1 - p) / p) ** (1 / ell))\n",
    "    return -((p / (1 - p)) ** (1 / ell))\n",
    "\n",
    "\n",
    "def reward(\n",
    "    scoring_rule: Callable[[float], float], true_bias: float, curr_pred: float\n",
    ") -> float:\n",
    "    return true_bias * scoring_rule(curr_pred) + (1 - true_bias) * scoring_rule(\n",
    "        1 - curr_pred\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_expert_flips(\n",
    "    scoring_rule: Callable[[float], float],\n",
    "    cost_per_flip: float,\n",
    "    num_trials: int = 1000,\n",
    "    ell: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Simulates expert behavior optimizing for ell-th moment of error.\n",
    "\n",
    "    Args:\n",
    "        scoring_rule: Function that takes prediction p and returns score\n",
    "        cost_per_flip: Cost c per coin flip\n",
    "        num_trials: Number of simulation trials\n",
    "        ell: Power of error moment to optimize\n",
    "\n",
    "    Returns:\n",
    "        Average ell-th power of absolute error across trials\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        true_bias = np.random.random()\n",
    "        heads = 0\n",
    "        flips = 0\n",
    "\n",
    "        while True:\n",
    "            curr_pred = (heads + 1) / (flips + 2)\n",
    "            curr_score = reward(scoring_rule, true_bias, curr_pred)\n",
    "\n",
    "            exp_heads = heads + true_bias\n",
    "            exp_flips = flips + 1\n",
    "            next_pred = (exp_heads + 1) / (exp_flips + 2)\n",
    "            next_score = reward(scoring_rule, true_bias, next_pred)\n",
    "\n",
    "            if next_score - curr_score < cost_per_flip:\n",
    "                break\n",
    "\n",
    "            if np.random.random() < true_bias:\n",
    "                heads += 1\n",
    "            flips += 1\n",
    "\n",
    "        errors.append(abs(curr_pred - true_bias) ** ell)\n",
    "\n",
    "    return np.mean(errors)\n",
    "\n",
    "\n",
    "def compare_scoring_rules(\n",
    "    cost_per_flip: float = 0.01, \n",
    "    num_trials: int = 1000,\n",
    "    ell_values: list = [1.0, 2.0, 4.0],\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares scoring rules across different ell values.\n",
    "\n",
    "    Args:\n",
    "        cost_per_flip: Cost c per coin flip\n",
    "        num_trials: Number of simulation trials \n",
    "        ell_values: List of ell values to test\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of results for each scoring rule and ell value\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for ell in ell_values:\n",
    "        scoring_rules = {\n",
    "            \"Quadratic\": quadratic_scoring_rule,\n",
    "            \"Logarithmic\": logarithmic_scoring_rule, \n",
    "            \"Spherical\": spherical_scoring_rule,\n",
    "            f\"Optimal (ell={ell})\": lambda p: optimal_scoring_rule(p, ell),\n",
    "        }\n",
    "\n",
    "        ell_results = {}\n",
    "        for name, rule in scoring_rules.items():\n",
    "            error = simulate_expert_flips(rule, cost_per_flip, num_trials, ell)\n",
    "            ell_results[name] = error\n",
    "\n",
    "        results[f\"ell={ell}\"] = ell_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_comparison(costs: list, ell: float = 2.0, num_trials: int = 1000):\n",
    "    \"\"\"\n",
    "    Plots error vs cost for different scoring rules.\n",
    "    \n",
    "    Args:\n",
    "        costs: List of cost values to test\n",
    "        ell: Power of error moment to optimize\n",
    "        num_trials: Number of simulation trials\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for c in costs:\n",
    "        result = compare_scoring_rules(c, num_trials=num_trials, ell_values=[ell])\n",
    "        results.append(result[f\"ell={ell}\"])\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for rule in results[0].keys():\n",
    "        errors = [r[rule] for r in results]\n",
    "        plt.plot(costs, errors, marker='o', label=rule)\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log') \n",
    "    plt.xlabel('Cost per flip')\n",
    "    plt.ylabel(f'Average {ell}-th power of absolute error')\n",
    "    plt.title(f'Scoring Rule Performance (ell={ell})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "plot_comparison([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
